

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Setup xCAT HA Mgmt with NFS pacemaker and corosync &mdash; xCAT 2.12 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="xCAT 2.12 documentation" href="../../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> xCAT
          

          
          </a>

          
            
            
              <div class="version">
                2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/install-guides/index.html">Install Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/admin-guides/index.html">Admin Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting/index.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developers/index.html">Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../help.html">Need Help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security/index.html">Security Notices</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">xCAT</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
    <li>Setup xCAT HA Mgmt with NFS pacemaker and corosync</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/advanced/hamn/setup_xcat_high_available_management_node_in_softlayer.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="setup-xcat-ha-mgmt-with-nfs-pacemaker-and-corosync">
<span id="setup-xcat-high-available-management-node-with-nfs"></span><h1>Setup xCAT HA Mgmt with NFS pacemaker and corosync<a class="headerlink" href="#setup-xcat-ha-mgmt-with-nfs-pacemaker-and-corosync" title="Permalink to this headline">¶</a></h1>
<p>In this doc, we will configure a xCAT HA cluster using <code class="docutils literal"><span class="pre">pacemaker</span></code> and <code class="docutils literal"><span class="pre">corosync</span></code> based on NFS server. <code class="docutils literal"><span class="pre">pacemaker</span></code> and <code class="docutils literal"><span class="pre">corosync</span></code> only support <code class="docutils literal"><span class="pre">x86_64</span></code> systems, more information about <code class="docutils literal"><span class="pre">pacemaker</span></code> and <code class="docutils literal"><span class="pre">corosync</span></code> refer to doc <a class="reference internal" href="setup_ha_mgmt_node_with_drbd_pacemaker_corosync.html#setup-ha-mgmt-node-with-drbd-pacemaker-corosync"><span>Setup HA Mgmt Node With DRBD Pacemaker Corosync</span></a>.</p>
<div class="section" id="prepare-environments">
<h2>Prepare environments<a class="headerlink" href="#prepare-environments" title="Permalink to this headline">¶</a></h2>
<p>The NFS SERVER IP is: c902f02x44 10.2.2.44</p>
<p>The NFS shares are <code class="docutils literal"><span class="pre">/disk1/install</span></code>, <code class="docutils literal"><span class="pre">/etc/xcat</span></code>, <code class="docutils literal"><span class="pre">/root/.xcat</span></code>, <code class="docutils literal"><span class="pre">/root/.ssh/</span></code>, <code class="docutils literal"><span class="pre">/disk1/hpcpeadmin</span></code></p>
<p>First xCAT Management node is: rhmn1 10.2.2.235</p>
<p>Second xCAT Management node is: rhmn2 10.2.2.233</p>
<p>Virtual IP: 10.2.2.150</p>
<p>This example will use static IP to provision nodes, so we do not use dhcp service. If you want to use dhcp service, you should consider to save dhcp related configuration files in NFS server.
The DB is SQLlite. There is no service node in this example.</p>
</div>
<div class="section" id="prepare-nfs-server">
<h2>Prepare NFS server<a class="headerlink" href="#prepare-nfs-server" title="Permalink to this headline">¶</a></h2>
<p>In NFS server 10.2.2.44, execute commands to export fs; If you want to use another non-root user to manage xCAT, such as hpcpeadmin.
You should create a directory for <code class="docutils literal"><span class="pre">/home/hpcpeadmin</span></code>; Execute commands in NFS server c902f02x44.</p>
<div class="highlight-python"><div class="highlight"><pre># service nfs start
# mkdir ~/.xcat
# mkdir -p /etc/xcat
# mkdir -p /disk1/install/
# mkdir -p /disk1/hpcpeadmin
# mkdir -p /disk1/install/xcat

# vi /etc/exports
/disk1/install *(rw,no_root_squash,sync,no_subtree_check)
/etc/xcat *(rw,no_root_squash,sync,no_subtree_check)
/root/.xcat *(rw,no_root_squash,sync,no_subtree_check)
/root/.ssh *(rw,no_root_squash,sync,no_subtree_check)
/disk1/hpcpeadmin *(rw,no_root_squash,sync,no_subtree_check)
# exportfs -a
</pre></div>
</div>
</div>
<div class="section" id="install-first-xcat-mn-rhmn1">
<h2>Install First xCAT MN rhmn1<a class="headerlink" href="#install-first-xcat-mn-rhmn1" title="Permalink to this headline">¶</a></h2>
<p>Execute steps on xCAT MN rhmn1</p>
<ol class="arabic">
<li><p class="first">Configure IP alias in rhmn1:</p>
<div class="highlight-python"><div class="highlight"><pre>ifconfig eth0:0 10.2.2.250 netmask 255.0.0.0
</pre></div>
</div>
</li>
<li><p class="first">Add alias ip into <code class="docutils literal"><span class="pre">/etc/resolv.conf</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>#vi /etc/resolv.conf
search pok.stglabs.ibm.com
nameserver 10.2.2.250
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">rsync</span></code> /etc/resolv.conf to <code class="docutils literal"><span class="pre">c902f02x44:/disk1/install/xcat/</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>rsync /etc/resolv.conf c902f02x44:/disk1/install/xcat/
</pre></div>
</div>
<p>Add alias ip，rhmn2,rhmn1 into <code class="docutils literal"><span class="pre">/etc/hosts</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>#vi /etc/hosts
10.2.2.233  rhmn2 rhmn2.pok.stglabs.ibm.com
10.2.2.235  rhmn1 rhmn1.pok.stglabs.ibm.com
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">rsync</span></code> /etc/hosts to <code class="docutils literal"><span class="pre">c902f02x44:/disk1/install/xcat/</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>rsync /etc/hosts c902f02x44:/disk1/install/xcat/
</pre></div>
</div>
</li>
<li><p class="first">Install first xcat MN rhmn1</p>
<p>Mount share nfs from 10.2.2.44:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># mkdir -p /install</span>
<span class="c"># mkdir -p /etc/xcat</span>
<span class="c"># mkdir -p /home/hpcpeadmin</span>
<span class="c"># mount 10.2.2.44:/disk1/install /install</span>
<span class="c"># mount 10.2.2.44:/etc/xcat /etc/xcat</span>
<span class="c"># mkdir -p /root/.xcat</span>
<span class="c"># mount 10.2.2.44:/root/.xcat /root/.xcat</span>
<span class="c"># mount 10.2.2.44:/root/.ssh /root/.ssh</span>
<span class="c"># mount 10.2.2.44:/disk1/hpcpeadmin /home/hpcpeadmin</span>
</pre></div>
</div>
<p>Create new user hpcpeadmin, change it password to hpcpeadminpw:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># USER=&quot;hpcpeadmin&quot;</span>
<span class="c"># GROUP=&quot;hpcpeadmin&quot;</span>
<span class="c"># /usr/sbin/groupadd -f ${GROUP}</span>
<span class="c"># /usr/sbin/useradd ${USER} -d /home/${USER} -s /bin/bash</span>
<span class="c"># /usr/sbin/usermod -a -G ${GROUP} ${USER}</span>
<span class="c"># passwd ${USER}</span>
</pre></div>
</div>
<p>Change new user hpcpeadmin as sudoers:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># USERNAME=&quot;hpcpeadmin&quot;</span>
<span class="c"># SUDOERS_FILE=&quot;/etc/sudoers&quot;</span>
<span class="c"># sed s&#39;/Defaults    requiretty/#Defaults    requiretty&#39;/g ${SUDOERS_FILE} &gt; /tmp/sudoers</span>
<span class="c"># echo &quot;$USERNAME ALL=(ALL) NOPASSWD:ALL&quot; &gt;&gt; /tmp/sudoers</span>
<span class="c"># cp -f /tmp/sudoers ${SUDOERS_FILE}</span>
<span class="c"># chown hpcpeadmin:hpcpeadmin /home/hpcpeadmin</span>
<span class="c"># rm -rf /tmp/sudoers</span>
</pre></div>
</div>
<p>Check the result:</p>
<div class="highlight-python"><div class="highlight"><pre>#su - hpcpeadmin
$ sudo cat /etc/sudoers|grep hpcpeadmin
hpcpeadmin ALL=(ALL) NOPASSWD:ALL
 $exit
</pre></div>
</div>
<p>Download xcat-core tar ball and xcat-dep tar ball from github, and untar them:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># mkdir /install/xcat</span>
<span class="c"># mv xcat-core-2.8.4.tar.bz2 /install/xcat/</span>
<span class="c"># mv xcat-dep-201404250449.tar.bz2 /install/xcat/</span>
<span class="c"># cd /install/xcat</span>
<span class="c"># tar -jxvf xcat-core-2.8.4.tar.bz2</span>
<span class="c"># tar -jxvf xcat-dep-201404250449.tar.bz2</span>
<span class="c"># cd xcat-core</span>
<span class="c"># ./mklocalrepo.sh</span>
<span class="c"># cd ../xcat-dep/rh6/x86_64/</span>
<span class="c"># ./mklocalrepo.sh</span>
<span class="c"># yum clean metadata</span>
<span class="c"># yum install xCAT</span>
<span class="c"># source /etc/profile.d/xcat.sh</span>
</pre></div>
</div>
</li>
<li><p class="first">Use vip in site table and networks table:</p>
<div class="highlight-python"><div class="highlight"><pre># chdef -t site master=10.2.2.250 nameservers=10.2.2.250
# chdef -t network 10_0_0_0-255_0_0_0 tftpserver=10.2.2.250
# tabdump networks
~]#netname,net,mask,mgtifname,gateway,dhcpserver,tftpserver,nameservers,ntpservers,logservers,dynamicrange,staticrange,staticrangeincrement,nodehostname,ddnsdomain,vlanid,domain,mtu,comments,disable
&quot;10_0_0_0-255_0_0_0&quot;,&quot;10.0.0.0&quot;,&quot;255.0.0.0&quot;,&quot;eth0&quot;,&quot;10.2.0.221&quot;,,&quot;10.2.2.250&quot;,,,,,,,,,,,,,
</pre></div>
</div>
</li>
<li><p class="first">Add 2 nodes into policy table:</p>
<div class="highlight-python"><div class="highlight"><pre>#tabedit policy
&quot;1.2&quot;,&quot;rhmn1&quot;,,,,,,&quot;trusted&quot;,,
&quot;1.3&quot;,&quot;rhmn2&quot;,,,,,,&quot;trusted&quot;,,
</pre></div>
</div>
</li>
<li><p class="first">Backup xcatDB(optional):</p>
<div class="highlight-python"><div class="highlight"><pre>dumpxCATdb -p &lt;yourbackupdir&gt;.
</pre></div>
</div>
</li>
<li><p class="first">Check and handle the policy table to allow the user to run commands:</p>
<div class="highlight-python"><div class="highlight"><pre># chtab policy.priority=6 policy.name=hpcpeadmin policy.rule=allow
# tabdump policy
/#priority,name,host,commands,noderange,parameters,time,rule,comments,disable
&quot;1&quot;,&quot;root&quot;,,,,,,&quot;allow&quot;,,
&quot;1.2&quot;,&quot;rhmn1&quot;,,,,,,&quot;trusted&quot;,,
&quot;1.3&quot;,&quot;rhmn2&quot;,,,,,,&quot;trusted&quot;,,
&quot;2&quot;,,,&quot;getbmcconfig&quot;,,,,&quot;allow&quot;,,
&quot;2.1&quot;,,,&quot;remoteimmsetup&quot;,,,,&quot;allow&quot;,,
&quot;2.3&quot;,,,&quot;lsxcatd&quot;,,,,&quot;allow&quot;,,
&quot;3&quot;,,,&quot;nextdestiny&quot;,,,,&quot;allow&quot;,,
&quot;4&quot;,,,&quot;getdestiny&quot;,,,,&quot;allow&quot;,,
&quot;4.4&quot;,,,&quot;getpostscript&quot;,,,,&quot;allow&quot;,,
&quot;4.5&quot;,,,&quot;getcredentials&quot;,,,,&quot;allow&quot;,,
&quot;4.6&quot;,,,&quot;syncfiles&quot;,,,,&quot;allow&quot;,,
&quot;4.7&quot;,,,&quot;litefile&quot;,,,,&quot;allow&quot;,,
&quot;4.8&quot;,,,&quot;litetree&quot;,,,,&quot;allow&quot;,,
&quot;6&quot;,&quot;hpcpeadmin&quot;,,,,,,&quot;allow&quot;,,
</pre></div>
</div>
</li>
<li><p class="first">Make sure xCAT commands are in the users path</p>
<div class="highlight-python"><div class="highlight"><pre># su - hpcpeadmin
$ echo $PATH | grep xcat
 /opt/xcat/bin:/opt/xcat/sbin:/opt/xcat/share/xcat/tools:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hpcpeadmin/bin
$lsdef -t site -l
</pre></div>
</div>
</li>
<li><p class="first">Stop the xcatd daemon and some related network services from starting on reboot</p>
<div class="highlight-python"><div class="highlight"><pre># service xcatd stop
Stopping xCATd [ OK ]
# chkconfig --level 345 xcatd off
# service conserver stop
conserver not running, not stopping [PASSED]
# chkconfig --level 2345 conserver off
# service dhcpd stop
# chkconfig --level 2345 dhcpd off
</pre></div>
</div>
<p>Remove the Virtual Alias IP</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># ifconfig eth0:0 0.0.0.0 0.0.0.0</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="install-second-xcat-mn-node-rhmn2">
<h2>Install second xCAT MN node rhmn2<a class="headerlink" href="#install-second-xcat-mn-node-rhmn2" title="Permalink to this headline">¶</a></h2>
<p>The installation steps are the exactly same with above part <code class="docutils literal"><span class="pre">Install</span> <span class="pre">fist</span> <span class="pre">xCAT</span> <span class="pre">MN</span> <span class="pre">node</span> <span class="pre">rhmn1</span></code>, using the same VIP with rhmn1.</p>
</div>
<div class="section" id="ssh-setup-across-nodes-rhmn1-and-rhmn2">
<h2>SSH Setup Across nodes rhmn1 and rhmn2<a class="headerlink" href="#ssh-setup-across-nodes-rhmn1-and-rhmn2" title="Permalink to this headline">¶</a></h2>
<p>Setup ssh across nodes rhmn1 and rhmn2, make sure rhmn1 can ssh to rhmn2 using no password:</p>
<div class="highlight-python"><div class="highlight"><pre>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
rsync -ave ssh /etc/ssh/ rhmn2:/etc/ssh/
rsync -ave ssh /root/.ssh/ rhmn2:/root/.ssh/
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">Note</span></code>: if they can ssh each other using password, it is enough.</p>
</div>
<div class="section" id="install-corosync-and-pacemaker-on-both-rhmn2-and-rhmn1">
<h2>Install corosync and pacemaker on both rhmn2 and rhmn1<a class="headerlink" href="#install-corosync-and-pacemaker-on-both-rhmn2-and-rhmn1" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">Download crmsh pssh python-pssh:</p>
<div class="highlight-python"><div class="highlight"><pre>wget download.opensuse.org/repositories/network:/ha-clustering:/Stable/RedHat_RHEL-6/x86_64/crmsh-2.1-1.1.x86_64.rpm
wget download.opensuse.org/repositories/network:/ha-clustering:/Stable/RedHat_RHEL-6/x86_64/pssh-2.3.1-4.2.x86_64.rpm
wget download.opensuse.org/repositories/network:/ha-clustering:/Stable/RedHat_RHEL-6/x86_64/python-pssh-2.3.1-4.2.x86_64.rpm
rpm -ivh python-pssh-2.3.1-4.2.x86_64.rpm
rpm -ivh pssh-2.3.1-4.2.x86_64.rpm
yum install redhat-rpm-config
rpm -ivh crmsh-2.1-1.1.x86_64.rpm
</pre></div>
</div>
</li>
<li><p class="first">Install <code class="docutils literal"><span class="pre">corosync</span></code> and <code class="docutils literal"><span class="pre">pacemaker</span></code> from OS repositories:</p>
<div class="highlight-python"><div class="highlight"><pre>#cd /etc/yum.repos.d
#cat rhel-local.repo
[rhel-local]
name=HPCCloud configured local yum repository for rhels6.5/x86_64
baseurl=http://10.2.0.221/install/rhels6.5/x86_64
enabled=1
gpgcheck=0

[rhel-local1]
name=HPCCloud1 configured local yum repository for rhels6.5/x86_64
baseurl=http://10.2.0.221/install/rhels6.5/x86_64/HighAvailability
enabled=1
gpgcheck=0
</pre></div>
</div>
</li>
<li><p class="first">Install <code class="docutils literal"><span class="pre">corosync</span></code> and <code class="docutils literal"><span class="pre">pacemaker</span></code>, then generate ssh key:</p>
<p>Install <code class="docutils literal"><span class="pre">corosync</span></code> and <code class="docutils literal"><span class="pre">pacemaker</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>yum install -y corosync pacemaker
</pre></div>
</div>
<p>Generate a Security Key, first generate a security key for authentication for all nodes in the cluster,
On one of the systems in the corosync cluster enter:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">corosync</span><span class="o">-</span><span class="n">keygen</span>
</pre></div>
</div>
<p>It will look like the command is not doing anything. It is waiting for entropy data
to be written to <code class="docutils literal"><span class="pre">/dev/random</span></code> until it gets 1024 bits. You can speed that process
up by going to another console for the system and entering:</p>
<div class="highlight-python"><div class="highlight"><pre>cd /tmp
wget http://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.32.8.tar.bz2
tar xvfj linux-2.6.32.8.tar.bz2
find .
</pre></div>
</div>
<p>This should create enough i/o, needed for entropy.
Then you need to copy that file to all of your nodes and put it in /etc/corosync/
with <code class="docutils literal"><span class="pre">user=root</span></code>, <code class="docutils literal"><span class="pre">group=root</span></code> and mode 0400:</p>
<div class="highlight-python"><div class="highlight"><pre>chmod 400 /etc/corosync/authkey
scp /etc/corosync/authkey vm2:/etc/corosync/
</pre></div>
</div>
</li>
<li><p class="first">Edit corosync.conf:</p>
<div class="highlight-python"><div class="highlight"><pre>#cat /etc/corosync/corosync.conf
#Please read the corosync.conf.5 manual page
 compatibility: whitetank
 totem {
    version: 2
    secauth: off
    threads: 0
    interface {
            member {
                  memberaddr: 10.2.2.233
                   }
            member {
                  memberaddr: 10.2.2.235
                   }
            ringnumber: 0
            bindnetaddr: 10.2.2.0
            mcastport: 5405
    }
    transport: udpu
 }
 logging {
    fileline: off
    to_stderr: no
    to_logfile: yes
    to_syslog: yes
    logfile: /var/log/cluster/corosync.log
    debug: off
    timestamp: on
    logger_subsys {
            subsys: AMF
            debug: off
    }
 }
 amf {
    mode: disabled
 }
</pre></div>
</div>
</li>
<li><p class="first">Configure <code class="docutils literal"><span class="pre">pacemaker</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre>#vi /etc/corosync/service.d/pcmk
service {
name: pacemaker
ver: 1
}
</pre></div>
</div>
</li>
<li><p class="first">Synchronize:</p>
<div class="highlight-python"><div class="highlight"><pre>for f in /etc/corosync/corosync.conf /etc/corosync/service.d/pcmk; do scp $f rhmn2:$f; done
</pre></div>
</div>
</li>
<li><p class="first">Start <code class="docutils literal"><span class="pre">corosync</span></code> and <code class="docutils literal"><span class="pre">pacemaker</span></code> in both rhmn1 and rhmn2:</p>
<div class="highlight-python"><div class="highlight"><pre># /etc/init.d/corosync start
Starting Corosync Cluster Engine (corosync): [ OK ]
# /etc/init.d/pacemaker start
Starting Pacemaker Cluster Manager[ OK ]
</pre></div>
</div>
</li>
<li><p class="first">Verify and let stonith false:</p>
<div class="highlight-python"><div class="highlight"><pre># crm_verify -L -V
error: unpack_resources: Resource start-up disabled since no STONITH resources have been defined
error: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option
error: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid
# crm configure property stonith-enabled=false
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="customize-corosync-pacemaker-configuration-for-xcat">
<h2>Customize corosync/pacemaker configuration for xCAT<a class="headerlink" href="#customize-corosync-pacemaker-configuration-for-xcat" title="Permalink to this headline">¶</a></h2>
<p>Be aware that you need to apply ALL the configuration at once. You cannot pick and choose which pieces to put in, and you cannot put some in now, and some later. Don&#8217;t execute individual commands, but use crm configure edit instead.</p>
<blockquote>
<div><p>Check that both rhmn2 and chetha are standby state now:</p>
<div class="highlight-python"><div class="highlight"><pre>rhmn1 ~]# crm status
Last updated: Wed Aug 13 22:57:58 2014
Last change: Wed Aug 13 22:40:31 2014 via cibadmin on rhmn1
Stack: classic openais (with plugin)
Current DC: rhmn2 - partition with quorum
Version: 1.1.8-7.el6-394e906
2 Nodes configured, 2 expected votes
14 Resources configured.
Node rhmn1: standby
Node rhmn2: standby
</pre></div>
</div>
<p>Execute <code class="docutils literal"><span class="pre">crm</span> <span class="pre">configure</span> <span class="pre">edit</span></code> to add all configure at once:</p>
<div class="highlight-python"><div class="highlight"><pre>rhmn1 ~]# crm configure edit
node rhmn1
node rhmn2 \
        attributes standby=on
primitive ETCXCATFS Filesystem \
        params device=&quot;10.2.2.44:/etc/xcat&quot; fstype=nfs options=v3 directory=&quot;/etc/xcat&quot; \
        op monitor interval=20 timeout=40
primitive HPCADMIN Filesystem \
        params device=&quot;10.2.2.44:/disk1/hpcpeadmin&quot; fstype=nfs options=v3     directory=&quot;/home/hpcpeadmin&quot; \
        op monitor interval=20 timeout=40
primitive ROOTSSHFS Filesystem \
        params device=&quot;10.2.2.44:/root/.ssh&quot; fstype=nfs options=v3 directory=&quot;/root/.ssh&quot; \
        op monitor interval=20 timeout=40
primitive INSTALLFS Filesystem \
        params device=&quot;10.2.2.44:/disk1/install&quot; fstype=nfs options=v3 directory=&quot;/install&quot; \
        op monitor interval=20 timeout=40
primitive NFS_xCAT lsb:nfs \
        op start interval=0 timeout=120s \
        op stop interval=0 timeout=120s \
        op monitor interval=41s
primitive NFSlock_xCAT lsb:nfslock \
        op start interval=0 timeout=120s \
        op stop interval=0 timeout=120s \
        op monitor interval=43s
primitive ROOTXCATFS Filesystem \
        params device=&quot;10.2.2.44:/root/.xcat&quot; fstype=nfs options=v3 directory=&quot;/root/.xcat&quot; \
        op monitor interval=20 timeout=40
primitive apache_xCAT apache \
        op start interval=0 timeout=600s \
        op stop interval=0 timeout=120s \
        op monitor interval=57s timeout=120s \
        params configfile=&quot;/etc/httpd/conf/httpd.conf&quot; statusurl=&quot;http://localhost:80/icons/README.html&quot; testregex=&quot;&lt;/html&gt;&quot; \
        meta target-role=Started
primitive dummy Dummy \
        op start interval=0 timeout=600s \
        op stop interval=0 timeout=120s \
        op monitor interval=57s timeout=120s \
        meta target-role=Started
primitive named lsb:named \
        op start interval=0 timeout=120s \
        op stop interval=0 timeout=120s \
        op monitor interval=37s
primitive dhcpd lsb:dhcpd \
        op start interval=&quot;0&quot; timeout=&quot;120s&quot; \
        op stop interval=&quot;0&quot; timeout=&quot;120s&quot; \
        op monitor interval=&quot;37s&quot;
primitive xCAT lsb:xcatd \
        op start interval=0 timeout=120s \
        op stop interval=0 timeout=120s \
        op monitor interval=42s \
        meta target-role=Started
primitive xCAT_conserver lsb:conserver \
        op start interval=0 timeout=120s \
        op stop interval=0 timeout=120s \
        op monitor interval=53
primitive xCATmnVIP IPaddr2 \
        params ip=10.2.2.250 cidr_netmask=8 \
        op monitor interval=30s
group XCAT_GROUP INSTALLFS ETCXCATFS ROOTXCATFS HPCADMIN ROOTSSHFS \
        meta resource-stickiness=100 failure-timeout=60 migration-threshold=3 target-role=Started
clone clone_named named \
        meta clone-max=2 clone-node-max=1 notify=false
colocation colo1 inf: NFS_xCAT XCAT_GROUP
colocation colo2 inf: NFSlock_xCAT XCAT_GROUP
colocation colo4 inf: apache_xCAT XCAT_GROUP
colocation colo7 inf: xCAT_conserver XCAT_GROUP
colocation dummy_colocation inf: dummy xCAT
colocation xCAT_colocation inf: xCAT XCAT_GROUP
colocation xCAT_makedns_colocation inf: xCAT xCAT_makedns
order Most_aftergrp inf: XCAT_GROUP ( NFS_xCAT NFSlock_xCAT apache_xCAT xCAT_conserver )
order Most_afterip inf: xCATmnVIP ( apache_xCAT xCAT_conserver )
order clone_named_after_ip_xCAT inf: xCATmnVIP clone_named
order dummy_order0 inf: NFS_xCAT dummy
order dummy_order1 inf: xCAT dummy
order dummy_order2 inf: NFSlock_xCAT dummy
order dummy_order3 inf: clone_named dummy
order dummy_order4 inf: apache_xCAT dummy
order dummy_order7 inf: xCAT_conserver dummy
order dummy_order8 inf: xCAT_makedns dummy
order xcat_makedns inf: xCAT xCAT_makedns
order dummy_order5 inf: dhcpd dummy
property cib-bootstrap-options: \
        dc-version=1.1.8-7.el6-394e906 \
        cluster-infrastructure=&quot;classic openais (with plugin)&quot; \
        expected-quorum-votes=2 \
        stonith-enabled=false \
        last-lrm-refresh=1406859140
\#vim:set syntax=pcmk
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="verify-auto-fail-over">
<h2>Verify auto fail over<a class="headerlink" href="#verify-auto-fail-over" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">Online rhmn1</p>
<p>Currently, rhmn2 and rhmn1 status are standby, let us online rhmn1:</p>
<div class="highlight-python"><div class="highlight"><pre>rhmn2 ~]# crm node online rhmn1
rhmn2 /]# crm status
Last updated: Mon Aug  4 23:16:44 2014
Last change: Mon Aug  4 23:13:09 2014 via crmd on rhmn2
Stack: classic openais (with plugin)
Current DC: rhmn1 - partition with quorum
Version: 1.1.8-7.el6-394e906
2 Nodes configured, 2 expected votes
12 Resources configured.
Node rhmn2: standby
Online: [ rhmn1 ]
Resource Group: XCAT_GROUP
     xCATmnVIP  (ocf::heartbeat:IPaddr2):       Started rhmn1
     INSTALLFS  (ocf::heartbeat:Filesystem):    Started rhmn1
     ETCXCATFS  (ocf::heartbeat:Filesystem):    Started rhmn1
     ROOTXCATFS (ocf::heartbeat:Filesystem):    Started rhmn1
NFS_xCAT       (lsb:nfs):      Started rhmn1
NFSlock_xCAT   (lsb:nfslock):  Started rhmn1
apache_xCAT    (ocf::heartbeat:apache):        Started rhmn1
xCAT   (lsb:xcatd):    Started rhmn1
xCAT_conserver (lsb:conserver):        Started rhmn1
dummy  (ocf::heartbeat:Dummy): Started rhmn1
 Clone Set: clone_named [named]
     Started: [ rhmn1 ]
     Stopped: [ named:1 ]
</pre></div>
</div>
</li>
<li><p class="first">xcat on rhmn2 is not working while it is running in rhmn1:</p>
<div class="highlight-python"><div class="highlight"><pre>rhmn2 /]# lsdef -t site -l
Unable to open socket connection to xcatd daemon on localhost:3001.
Verify that the xcatd daemon is running and that your SSL setup is correct.
Connection failure: IO::Socket::INET: connect: Connection refused at /opt/xcat/lib/perl/xCAT/Client.pm line 217.

rhmn2 /]# ssh rhmn1 &quot;lsxcatd -v&quot;
Version 2.8.4 (git commit 7306ca8abf1c6d8c68d3fc3addc901c1bcb6b7b3, built Mon Apr 21 20:48:59 EDT 2014)
</pre></div>
</div>
</li>
<li><p class="first">Let rhmn1 standby and rhmn2 online, xcat will run on rhmn2:</p>
<div class="highlight-python"><div class="highlight"><pre>rhmn2 /]# crm node online rhmn2
rhmn2 /]# crm node standby rhmn1
rhmn2 /]# crm status
Last updated: Mon Aug 4 23:19:33 2014
Last change: Mon Aug 4 23:19:40 2014 via crm_attribute on rhmn2
Stack: classic openais (with plugin)
Current DC: rhmn1 - partition with quorum
Version: 1.1.8-7.el6-394e906
2 Nodes configured, 2 expected votes
12 Resources configured.

Node rhmn1: standby
Online: [ rhmn2 ]

Resource Group: XCAT_GROUP
xCATmnVIP (ocf::heartbeat:IPaddr2): Started rhmn2
INSTALLFS (ocf::heartbeat:Filesystem): Started rhmn2
ETCXCATFS (ocf::heartbeat:Filesystem): Started rhmn2
ROOTXCATFS (ocf::heartbeat:Filesystem): Started rhmn2
NFSlock_xCAT (lsb:nfslock): Started rhmn2
xCAT (lsb:xcatd): Started rhmn2
Clone Set: clone_named [named]
Started: [ rhmn2 ]
Stopped: [ named:1 ]

rhmn2 /]#lsxcatd -v
Version 2.8.4 (git commit 7306ca8abf1c6d8c68d3fc3addc901c1bcb6b7b3, built Mon Apr 21 20:48:59 EDT 2014)
</pre></div>
</div>
</li>
</ol>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, IBM Corporation.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'2.12',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>